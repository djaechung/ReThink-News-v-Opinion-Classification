{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19f87303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import collections\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bface",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c528fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cleaned, labelled article data as a Pandas dataframe\n",
    "data = pd.read_excel(\"Labelled_VR_data_Oct2020_Jan2021_wfulltext.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d42f3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting article status strings to article status indicators.\n",
    "# 1 = Opinion, 0 = News\n",
    "def indicator(art_status):\n",
    "    if art_status == \"News\":\n",
    "        return 0\n",
    "    elif art_status == \"Opinion\":\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Warning: art_status is neither news nor opinion\")\n",
    "        return -1\n",
    "    \n",
    "# Applying article indicator conversion function to the dataframe    \n",
    "data[\"Article Status Int\"] = data[\"Article Status\"].apply(indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0d24804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spltting the article data by news and opinion label\n",
    "news = data[data[\"Article Status\"] == \"News\"]\n",
    "opinion = data[data[\"Article Status\"] == \"Opinion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562857b",
   "metadata": {},
   "source": [
    "# Shiny's Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27c9edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features —  SENTLENGTH and TOKENLENGTH\n",
    "#hypothesis being that opinion texts, such as editorials, tend to feature longer sentences \n",
    "#art_str is the article as a String\n",
    "\n",
    "def sent_len(art_str):\n",
    "    \"\"\"\n",
    "    Returns the average sentence length measured in tokens (inverted).\n",
    "    \"\"\"\n",
    "    dots = [p for p in range(len(art_str)) if art_str[p] == \".\"]\n",
    "    sentences = [dots[i+1] - dots[i] for i in range(len(dots) - 1)]\n",
    "    return (1/np.average(sentences)) if sentences else (1/len(art_str))\n",
    "    \n",
    "def token_len(art_str):\n",
    "    \"\"\"\n",
    "    Returns the average token length measured in characters (inverted).\n",
    "    \"\"\"\n",
    "    wordList = art_str.split()\n",
    "    lengths = [len(w) for w in wordList]\n",
    "    return 1/(np.average(lengths)), np.sum(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "939d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — NEGATION and NEGATIONSUFFIX \n",
    "#NEGATION — OPINION\n",
    "#NEGATIONSUFFIX — NEWS\n",
    "#hypothesis being that opinion texts tend to have more negations \n",
    "#n't is used commonly in citations or quotes\n",
    "\n",
    "negations = [\"no\", \"not\", \"none\", \"no one\", \"nobody\", \"neither\", \"nowhere\", \"nothing\", \"never\"]\n",
    "neg_suffix = \"n\\'t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd7390ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negations_count(art_str):\n",
    "    wordList = art_str.lower().split()\n",
    "    total, suffix_total = 0, 0\n",
    "    for w in wordList:\n",
    "        if w in negations:\n",
    "            total += 1\n",
    "        if w[-3:] == neg_suffix:\n",
    "            suffix_total += 1\n",
    "    return total, suffix_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5be2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — QUESTIONS, EXCLAMATIONS, COMMAS, and SEMICOLONS\n",
    "#hypothesis being that opinion texts tend to use more exclamations and (rhetorical) questions\n",
    "#exclamation marks, question marks, semicolons, and commas\n",
    "#QUESTIONS, EXCLAMATIONS, COMMAS, SEMICOLONS — OPINION\n",
    "#COMMAS — NEWS\n",
    " \n",
    "def punctuation_count(art_str):\n",
    "    \"\"\"\n",
    "    Determines the numbers of exclamation marks, question marks, semicolons, and commas,\n",
    "    as compared to other punctuation symbols. \n",
    "    \"\"\"\n",
    "    count = [0] * 5\n",
    "    for i in range(len(art_str)):\n",
    "        if art_str[i] == \"?\":\n",
    "            count[0] += 1\n",
    "        elif art_str[i] == \"!\":\n",
    "            count[1] += 1\n",
    "        elif art_str[i] == \",\":\n",
    "            count[2] += 1\n",
    "        elif art_str[i] == \";\":\n",
    "            count[3] += 1\n",
    "        elif art_str[i] == \".\":\n",
    "            count[4] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "287c421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — CONNECTIVES (Temporal, Casual, Contrastive, Expansive)\n",
    "#hypothesis being that there are more connectives in news (aftermath of study)\n",
    "#casual, expansive, temporal, contrastive — NEWS \n",
    "\n",
    "\n",
    "casual = ['after', 'because', 'insofar as', 'by', 'in turn', 'for', 'once', 'as a result', 'hence', 'in the end', \n",
    "          'by then', 'but', 'subsequently', 'as', 'therefore', 'unless', 'thus', 'accordingly', 'so that', 'since', \n",
    "          'consequently', 'indeed', 'ultimately', 'then', 'even though', 'now that', 'finally”,”hence”,”if', 'although', \n",
    "          'so', 'thereby', 'otherwise', 'due to', 'and', 'when']\n",
    "\n",
    "contrastive = ['nor', 'in fact', 'despite', 'equally', 'by comparison', 'contrast', 'by contrast', 'but', 'separately', \n",
    "               'whereas', 'rather', 'meanwhile', 'also', 'even so', 'and', 'though', 'if', 'unlike', 'however', 'or', 'then', \n",
    "               'nevertheless', 'yet', 'even though', 'conversely', 'nonetheless', 'on the contrary', 'in contrast', 'while', \n",
    "               'likewise', 'instead', 'although', 'on the other hand', 'still', 'similarly', 'otherwise', 'actually', \n",
    "               'alternatively', 'on the one hand', 'when']\n",
    "\n",
    "temporal = [\"before\", \"after\", \"next\", \"shortly\", \"afterwards\", \"eventually\", \"firstly\", \"secondly\", \"previously\", \"meanwhile\",\n",
    "            \"finally\", \"while\", \"then\", \"earlier\", \"when\", \"initially\", \"soon\", \"suddenly\", \"until\", \"once\", \"recently\", \"already\", \"as\"]\n",
    "\n",
    "expansive = [\"also\", \"and\", \"as well as\", \"besides\", \"in addition\", \"furthermore\", \"in fact\", \"moreover\", \"additionally\",\n",
    "             \"too\", \"further\", \"or\", \"neither\", \"nor\", \"either\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2699079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connective_count(art_str):\n",
    "    # casual, contrastive, temporal, expansive\n",
    "    connectives = [0] * 4\n",
    "    wordList = art_str.lower().split()\n",
    "    for w in wordList:\n",
    "        if w in casual:\n",
    "            connectives[0] += 1\n",
    "        elif w in contrastive:\n",
    "            connectives[1] += 1\n",
    "        elif w in temporal:\n",
    "            connectives[2] += 1\n",
    "        elif w in expansive:\n",
    "            connectives[3] += 1\n",
    "    return connectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5081ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — PRONOUNS outside of quotes — OPINION\n",
    "#study used first and second person only\n",
    "\n",
    "\n",
    "first_person = ['I', 'we', 'our', 'ourselves', 'us', 'me', 'my', 'mine', 'myself']\n",
    "second_person = ['you', 'yours', 'your', 'yourself', 'yourselves']\n",
    "third_person = ['he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "                'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7294f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronouns_count(art_str):\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  art_str).split()\n",
    "    pronoun_count = [0] * 4\n",
    "    for w in wordList:\n",
    "        \"\"\"0: 1st person\n",
    "           1: 2nd person\n",
    "           2: 3rd person\n",
    "           3: 1st and 2nd person\"\"\"\n",
    "        w_lower = w.lower()\n",
    "        if w_lower in first_person:\n",
    "            pronoun_count[0] += 1\n",
    "            pronoun_count[3] += 1\n",
    "        elif w_lower in second_person:\n",
    "            pronoun_count[1] += 1\n",
    "            pronoun_count[3] += 1\n",
    "        elif w_lower in third_person:\n",
    "            pronoun_count[2] += 1\n",
    "    return pronoun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55d769e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — CITATIONS and CITATIONLENGTH — OPINION\n",
    "#hypothesis being that higher frequencies of citations are more indicative of opinion (aftermath of study)\n",
    "\n",
    "def find_citation(art_str):\n",
    "    \"\"\"\n",
    "    Returns the citation length and frequency in the article. \n",
    "    \"\"\"\n",
    "    quotes = re.findall(r'\"(.*?)\"', art_str)\n",
    "    within_quotes = collections.Counter(\" \".join(quotes).lower().split())\n",
    "    num_citations = len(quotes)\n",
    "    avg_citation_len = np.average([len(q) for q in quotes])\n",
    "    return [num_citations, avg_citation_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c9e4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — MODALS — OPINION\n",
    "#hypothesis being that modal verbs are expected to be a better indicator for opinion \n",
    "#according to study, is less potent but do correspond to the genre as expected\n",
    "\n",
    "modals = [\"can\", \"must\", \"may\", \"could\", \"might\", \"should\", \"would\", \"shall\", \"ought to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ef59db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — VERBSofSAYING (VoS) — NEWS\n",
    "#hypothesis that vos more common in news\n",
    "\n",
    "vos = ['acknowledge','affirm','allege','announce','assert','claim','comment','contend','declare','disclose',\n",
    "       'exclaim','explain','insist','mention','notify','observe','proclaim','propose','report','reveal','said','say','state',\n",
    "       'stipulate','tell','write']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63cb7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — FUTURE_WILL — NEWS\n",
    "#frequency of the verb, \"will\" outside of quotes\n",
    "\n",
    "future_will = \"will\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3a5c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — DIGITS — OPINION\n",
    "#hypothesis being that digits are more frequent in opinion than news (aftermath of study)\n",
    "\n",
    "def count_digits(art_str):\n",
    "    \"\"\"\n",
    "    Returns the frequency of digits in a text. \n",
    "    \"\"\"\n",
    "    return len(re.findall(\"[\\d]+\",art_str))/len(art_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfd808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_verb_tags = [\"VBD\", \"VBP\", \"VBZ\", \"MD\", \"BES\", \"HVS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "daf03b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complexity and finite verbs\n",
    "\n",
    "def get_finite_verbs(art_str):\n",
    "    finite_verb_tags = [\"VBD\", \"VBP\", \"VBZ\", \"MD\", \"BES\", \"HVS\"]\n",
    "    return [w for w in nlp(art_str) if w.tag_ in finite_verb_tags]\n",
    "\n",
    "def calc_complexity(art_str):\n",
    "    num_finite_verbs = len(get_finite_verbs(art_str))\n",
    "    complexity = num_finite_verbs / token_len(art_str)[1]\n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfceb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#present and past tense frequency\n",
    "\n",
    "def present_tense_freq(finite_verbs):\n",
    "    present_verbs = [w for w in finite_verbs if w.tag_ == \"VBZ\" or w.tag_ == \"VBP\"]\n",
    "    present_tense_frequency = len(present_verbs) / len(finite_verbs)\n",
    "    return present_tense_frequency\n",
    "\n",
    "def past_tense_freq(finite_verbs):\n",
    "    past_verbs = [w for w in finite_verbs if w.tag_ == \"VBD\"]\n",
    "    past_tense_frequency = len(past_verbs) / len(finite_verbs)\n",
    "    return past_tense_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ad619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interjection frequency\n",
    "\n",
    "def get_interjections(art_str):\n",
    "    interjections = [w for w in nlp(art_str) if w.pos_ == \"INTJ\"]\n",
    "    intj_freq = len(interjections) / token_len(art_str)[1]\n",
    "    return intj_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37d75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778\n"
     ]
    }
   ],
   "source": [
    "# Subjectivity Dictionary\n",
    "import json\n",
    "mpqa_dict = json.load(open(\"mpqa_dict.json\", \"r\"))\n",
    "\n",
    "print(len(mpqa_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2467289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistic features — SENTIMENT — OPINION\n",
    "#hypothesis being that opinion texts employ a less neutral language\n",
    "#calculated in study using MPQA Subjectivity Clues Lexicon \n",
    "\n",
    "def get_sentiment(art_str, mpqa_dict):\n",
    "    subjectivity, adjectives = 0, 0\n",
    "    words = art_str.lower().split()\n",
    "    \n",
    "    for w in words:\n",
    "        if w in mpqa_dict:  \n",
    "            if mpqa_dict[w]['pos'] == 'adj':\n",
    "                adjectives += 1\n",
    "            if mpqa_dict[w]['subj'] == 'weaksubj':\n",
    "                subjectivity += 0.1\n",
    "            if mpqa_dict[w]['subj'] == 'strongsubj':\n",
    "                subjectivity += 1\n",
    "          \n",
    "    num_words = len(words)\n",
    "    sentiment = subjectivity / num_words\n",
    "    adj_ratio = adjectives / num_words\n",
    "    \n",
    "    return subjectivity, sentiment, adjectives, adj_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80e40536",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pattern = \"'[^']*'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5dc7c",
   "metadata": {},
   "source": [
    "# David's Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16c8d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning string-based columns by filling na values with empty strings\n",
    "data[\"Full Text\"].fillna(\"\");\n",
    "data[\"Journalist Name\"].fillna(\"\");\n",
    "data[\"Headline\"].fillna(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c5a7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headline length feature\n",
    "#On average, news headlines are slightly longer than opinion headlines\n",
    "#df.groupby([\"news_opinion\"])[\"headline_length\"].mean()\n",
    "data[\"headline_length_feature\"] = data[\"Headline\"].str.len()\n",
    "\n",
    "#Author count feature\n",
    "#News articles tend to have slightly more authors\n",
    "#df.groupby([\"news_opinion\",\"author_count\"]).size()\n",
    "data[\"author_count_feature\"] = data[\"Cleaned Author\"].str.count(\", \") + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "daf4850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "a = data[\"Headline\"].str.len()\n",
    "b = data[\"Cleaned Author\"].str.count(\", \") + 1\n",
    "zipped = list(zip(a,b))\n",
    "fitted = scaler.fit(zipped)\n",
    "arr = scaler.transform(zipped)\n",
    "t = zip(*arr)\n",
    "new = list(t)\n",
    "data['minmax_length'] = new[0]\n",
    "data['minmax_author'] = new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94ce2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['minmax_length'] = data['minmax_length'].fillna(0);\n",
    "data['minmax_author'] = data['minmax_author'].fillna(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9defa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = max(data[\"headline_length_feature\"])\n",
    "normalized1 = data[\"headline_length_feature\"]/m1\n",
    "data['normalized_length'] = normalized1\n",
    "m2 = max(data[\"author_count_feature\"])\n",
    "normalized2 = data[\"author_count_feature\"]/m2\n",
    "data['normalized_author_count'] = normalized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fc0507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalized_length'] = data['normalized_length'].fillna(0);\n",
    "data['normalized_author_count'] = data['normalized_author_count'].fillna(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5631001",
   "metadata": {},
   "source": [
    "# Combined Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f87dc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(record, df):\n",
    "    \n",
    "    # Features with no for loops\n",
    "    art_str = record[\"Full Text\"]\n",
    "    minmax_length = record[\"minmax_length\"]\n",
    "    minmax_author = record[\"minmax_author\"]\n",
    "    normalized_length = record[\"normalized_length\"]\n",
    "    normalized_author_count = record[\"normalized_author_count\"]\n",
    "    \n",
    "    counter = collections.Counter(art_str.lower().split())\n",
    "    num_words = sum(list(counter.values()))\n",
    "    \n",
    "    quotes = re.findall(r'\"(.*?)\"', art_str)\n",
    "    within_quotes = collections.Counter(\" \".join(quotes).lower().split())\n",
    "    num_citations = len(quotes)\n",
    "    avg_citation_len = np.average([len(q) for q in quotes]) if quotes else 0\n",
    "    sent_length = sent_len(art_str)\n",
    "    token_length, num_tokens = token_len(art_str) \n",
    "    \n",
    "    # Things we need to count\n",
    "    num_modals, num_vos = 0, 0\n",
    "    num_casual, num_temporal, num_contrastive, num_expansive = 0, 0, 0, 0\n",
    "    first_p, second_p, third_p, first_second_p = 0, 0, 0, 0\n",
    "    num_future = counter.get(\"will\") or 0\n",
    "    opinion_count, news_count = 0, 0\n",
    "    num_negation, negation_suffix = 0, 0\n",
    "    num_finite_verbs = 0\n",
    "    digits = 0\n",
    "    num_past_tense, num_pres_tense = 0, 0\n",
    "    num_intjs = 0\n",
    "    \n",
    "    # Features with for loops\n",
    "    for w in counter.keys():\n",
    "        if w in vos:\n",
    "            num_vos += counter.get(w)\n",
    "        if w in first_person:\n",
    "            first_p += counter.get(w)\n",
    "            first_second_p += counter.get(w)\n",
    "        if w in second_person:\n",
    "            second_p += counter.get(w)\n",
    "            first_second_p += counter.get(w)\n",
    "        if w in third_person:\n",
    "            third_p += counter.get(w)\n",
    "        if w in modals:\n",
    "            num_modals += counter.get(w)\n",
    "        if w in casual:\n",
    "            num_casual += counter.get(w)\n",
    "        if w in temporal:\n",
    "            num_temporal += counter.get(w)\n",
    "        if w in contrastive:\n",
    "            num_contrastive += counter.get(w)\n",
    "        if w in expansive:\n",
    "            num_expansive += counter.get(w)\n",
    "        if any(char.isdigit() for char in w):\n",
    "            digits += 1\n",
    "        if w in opinion:\n",
    "            opinion_count += counter.get(w)\n",
    "        if w in news:\n",
    "            news_count += counter.get(w)\n",
    "        if \"n't\" in w:\n",
    "            negation_suffix += counter.get(w)\n",
    "        if w in negations:\n",
    "            num_negation += counter.get(w)\n",
    "    \n",
    "    for w in nlp(art_str):\n",
    "        if w.tag_ in finite_verb_tags:\n",
    "            num_finite_verbs += 1\n",
    "        if w.tag_ == \"VBZ\" or w.tag_ == \"VBP\":\n",
    "            num_pres_tense += 1\n",
    "        if w.tag_ == \"VBD\":\n",
    "            num_past_tense += 1\n",
    "    \n",
    "    # Other \n",
    "    if num_tokens == 0:\n",
    "        comlexity, interjection_freq = 0, 0\n",
    "    else: \n",
    "        complexity = num_finite_verbs / num_tokens\n",
    "        interjection_freq = num_intjs / num_tokens\n",
    "    if num_finite_verbs == 0:\n",
    "         past_freq, present_freq = 0, 0\n",
    "    else:\n",
    "        past_freq = num_past_tense / num_finite_verbs\n",
    "        present_freq = num_pres_tense / num_finite_verbs\n",
    "    \n",
    "    subjectivity, sentiment, adjectives, adj_ratio = get_sentiment(art_str, mpqa_dict)\n",
    "    questions, exclamations, semicolons, commas, periods = punctuation_count(art_str)\n",
    "    \n",
    "    # Compilation\n",
    "    ling_features = [sent_length, token_length, first_p, second_p, third_p, first_second_p,\n",
    "                    questions, exclamations, semicolons, commas, periods, \n",
    "                    num_casual, num_temporal, num_contrastive, num_expansive, \n",
    "                    digits, num_modals, num_vos, num_future, \n",
    "                    opinion_count, news_count,\n",
    "                    num_negation, negation_suffix, num_citations, avg_citation_len, num_words,\n",
    "                    subjectivity, sentiment, adjectives, adj_ratio,\n",
    "                    minmax_length, minmax_author, normalized_length, normalized_author_count,\n",
    "                    complexity, present_freq, past_freq, interjection_freq]\n",
    "    return ling_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fe209",
   "metadata": {},
   "source": [
    "# Feature Extraction Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a47721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 3535\n"
     ]
    }
   ],
   "source": [
    "# Preparing to create a dataframe where each record is a full text, and each column is a feature\n",
    "\n",
    "# Collecting the label for each record in an array\n",
    "y = []\n",
    "count = 1\n",
    "total = len(data)\n",
    "for article_label in data[\"Article Status Int\"]:\n",
    "    y.append(article_label)\n",
    "y = np.array(y)\n",
    "\n",
    "# Collecting the features for each record in an array\n",
    "X = []\n",
    "for index, record in data.iterrows():\n",
    "    article_features = get_all_features(record, data)\n",
    "    X.append(article_features)\n",
    "    \n",
    "    if count == 101: # Stopper so that we only extract the features of the first 101 records. (For time's sake)\n",
    "        break\n",
    "    if count%100 == 0:\n",
    "        print(str(count) + \" of \" + str(total))\n",
    "    count += 1\n",
    "    \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e60ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a reference for the features and target variables we are considering\n",
    "X_cols = ['sent_length', 'token_length', 'first_p', 'second_p', 'third_p', \"first/second_p\",\n",
    "                    'questions', 'exclamations', 'semicolons', 'commas', 'periods', \n",
    "                    'num_casual', 'num_temporal', 'num_contrastive', 'num_expansive', \n",
    "                    'digits', 'num_modals', 'num_vos', 'num_future', \n",
    "                    'opinion_count', 'news_count',\n",
    "                    'num_negation', 'negation_suffix', 'num_citations', 'avg_citation_len', 'num_words',\n",
    "                    'subjectivity', 'sentiment', 'adjectives', 'adj_ratio',\n",
    "                    'minmax_length', 'minmax_author', 'normalized_length', 'normalized_author_count', \n",
    "                    'complexity', 'present', 'past', 'interjections']\n",
    "y_col = ['art_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6bff8392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>first_p</th>\n",
       "      <th>second_p</th>\n",
       "      <th>third_p</th>\n",
       "      <th>first/second_p</th>\n",
       "      <th>questions</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>semicolons</th>\n",
       "      <th>commas</th>\n",
       "      <th>...</th>\n",
       "      <th>adj_ratio</th>\n",
       "      <th>minmax_length</th>\n",
       "      <th>minmax_author</th>\n",
       "      <th>normalized_length</th>\n",
       "      <th>normalized_author_count</th>\n",
       "      <th>complexity</th>\n",
       "      <th>present</th>\n",
       "      <th>past</th>\n",
       "      <th>interjections</th>\n",
       "      <th>art_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036988</td>\n",
       "      <td>0.142232</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.156989</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.018143</td>\n",
       "      <td>0.316176</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.190626</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051511</td>\n",
       "      <td>0.105033</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120430</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.017806</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.200954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022152</td>\n",
       "      <td>0.155361</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.169892</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.015262</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.187560</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.155361</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.169892</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.187835</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>0.113786</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.016696</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.189315</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036209</td>\n",
       "      <td>0.056893</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.073118</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.089716</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>0.553488</td>\n",
       "      <td>0.283721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.206884</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039405</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.191883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>0.122538</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.010747</td>\n",
       "      <td>0.193126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021116</td>\n",
       "      <td>0.216630</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.230108</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sent_length  token_length  first_p  second_p  third_p  first/second_p  \\\n",
       "0       0.014900      0.201974      1.0       2.0     44.0             3.0   \n",
       "1       0.007716      0.190626      9.0       3.0     51.0            12.0   \n",
       "2       0.021549      0.200954      1.0       0.0     21.0             1.0   \n",
       "3       0.007221      0.187560      5.0       0.0     20.0             5.0   \n",
       "4       0.006043      0.187835      1.0       0.0     20.0             1.0   \n",
       "..           ...           ...      ...       ...      ...             ...   \n",
       "96      0.007687      0.189315     36.0       0.0     14.0            36.0   \n",
       "97      0.006857      0.199559      9.0       1.0     57.0            10.0   \n",
       "98      0.007088      0.206884     25.0       1.0     38.0            26.0   \n",
       "99      0.007906      0.191883      0.0       0.0     34.0             0.0   \n",
       "100     0.010747      0.193126      0.0       2.0     23.0             2.0   \n",
       "\n",
       "     questions  exclamations  semicolons  commas  ...  adj_ratio  \\\n",
       "0          0.0           0.0        78.0     3.0  ...   0.036988   \n",
       "1          1.0           0.0        92.0     3.0  ...   0.051511   \n",
       "2          0.0           0.0        30.0     1.0  ...   0.022152   \n",
       "3          0.0           0.0        31.0     1.0  ...   0.044643   \n",
       "4          0.0           0.0        50.0     0.0  ...   0.025397   \n",
       "..         ...           ...         ...     ...  ...        ...   \n",
       "96         0.0           0.0        55.0     0.0  ...   0.036209   \n",
       "97         0.0           0.0       106.0     2.0  ...   0.029633   \n",
       "98         3.0           0.0        55.0     1.0  ...   0.039405   \n",
       "99         0.0           0.0        38.0     1.0  ...   0.024551   \n",
       "100        0.0           0.0        25.0     0.0  ...   0.021116   \n",
       "\n",
       "     minmax_length  minmax_author  normalized_length  normalized_author_count  \\\n",
       "0         0.142232           0.00           0.156989                      0.2   \n",
       "1         0.105033           0.00           0.120430                      0.2   \n",
       "2         0.155361           0.25           0.169892                      0.4   \n",
       "3         0.155361           0.00           0.169892                      0.2   \n",
       "4         0.113786           0.25           0.129032                      0.4   \n",
       "..             ...            ...                ...                      ...   \n",
       "96        0.056893           0.00           0.073118                      0.2   \n",
       "97        0.089716           0.00           0.105376                      0.2   \n",
       "98        0.087527           0.00           0.103226                      0.2   \n",
       "99        0.122538           0.00           0.137634                      0.0   \n",
       "100       0.216630           0.00           0.230108                      0.2   \n",
       "\n",
       "     complexity   present      past  interjections  art_status  \n",
       "0      0.018143  0.316176  0.602941            0.0           0  \n",
       "1      0.017806  0.720588  0.125000            0.0           0  \n",
       "2      0.015262  0.125000  0.770833            0.0           0  \n",
       "3      0.013397  0.446429  0.178571            0.0           1  \n",
       "4      0.016696  0.523810  0.321429            0.0           0  \n",
       "..          ...       ...       ...            ...         ...  \n",
       "96     0.014516  0.611111  0.180556            0.0           1  \n",
       "97     0.021550  0.553488  0.283721            0.0           0  \n",
       "98     0.020290  0.482143  0.267857            0.0           1  \n",
       "99     0.015764  0.379310  0.494253            0.0           0  \n",
       "100    0.018351  0.174603  0.698413            0.0           0  \n",
       "\n",
       "[101 rows x 39 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe where each record is a full text, and each column is a feature\n",
    "data_wfeats = pd.DataFrame(X, columns=X_cols).head(101) # We're only considering the first 101 records for this demo\n",
    "data_wfeats = data_wfeats[X_cols]\n",
    "data_wfeats['art_status'] = y[:101] # Accounting for us only considering the first 101 records for this demo\n",
    "data_wfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de2d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce6988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
